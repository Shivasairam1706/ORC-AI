version: '3.8'

services:
  # PostgreSQL Database for Airflow metadata
  postgres:
    image: postgres:15
    container_name: orc-ai-postgres
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-airflow}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-airflow}
      POSTGRES_DB: ${POSTGRES_DB:-airflow}
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 10s
      retries: 5
      start_period: 5s
    restart: always
    networks:
      - orc-ai-network

  # Redis Message Broker for Celery
  redis:
    image: redis:7-alpine
    container_name: orc-ai-redis
    expose:
      - 6379
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 30s
      retries: 50
      start_period: 30s
    restart: always
    networks:
      - orc-ai-network

  # Neo4j Knowledge Graph Database
  neo4j:
    image: neo4j:5.15-community
    container_name: orc-ai-neo4j
    environment:
      NEO4J_AUTH: ${NEO4J_USER:-neo4j}/${NEO4J_PASSWORD:-password}
      NEO4J_PLUGINS: '["apoc", "graph-data-science"]'
      NEO4J_apoc_export_file_enabled: 'true'
      NEO4J_apoc_import_file_enabled: 'true'
      NEO4J_apoc_import_file_use__neo4j__config: 'true'
      NEO4J_ACCEPT_LICENSE_AGREEMENT: 'yes'
    volumes:
      - neo4j_data:/data
      - neo4j_logs:/logs
      - neo4j_conf:/conf
      - neo4j_import:/import
      - ./podman/data:/var/lib/neo4j/import/data
    ports:
      - "7474:7474"  # HTTP
      - "7687:7687"  # Bolt
    healthcheck:
      test: ["CMD", "cypher-shell", "-u", "neo4j", "-p", "password", "RETURN 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 20s
    restart: always
    networks:
      - orc-ai-network

  # Airflow Webserver
  airflow-webserver:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: orc-ai-airflow-webserver
    command: webserver
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow
      AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW_FERNET_KEY}
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session'
      AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: 'true'
      _AIRFLOW_DB_UPGRADE: 'true'
      _AIRFLOW_WWW_USER_CREATE: 'true'
      _AIRFLOW_WWW_USER_USERNAME: ${AIRFLOW_USERNAME:-admin}
      _AIRFLOW_WWW_USER_PASSWORD: ${AIRFLOW_PASSWORD:-admin}
      OPENAI_API_KEY: ${OPENAI_API_KEY}
      ANTHROPIC_API_KEY: ${ANTHROPIC_API_KEY}
      GOOGLE_API_KEY: ${GOOGLE_API_KEY}
      MISTRAL_API_KEY: ${MISTRAL_API_KEY}
    volumes:
      - ${AIRFLOW_PROJ_DIR:-.}/podman/code:/opt/airflow/dags
      - ${AIRFLOW_PROJ_DIR:-.}/podman/code/plugins:/opt/airflow/plugins
      - ${AIRFLOW_PROJ_DIR:-.}/podman/temp:/opt/airflow/logs
      - ${AIRFLOW_PROJ_DIR:-.}/podman/data:/opt/airflow/data
      - ${AIRFLOW_PROJ_DIR:-.}/podman/model:/opt/airflow/models
      - ${AIRFLOW_PROJ_DIR:-.}/podman/test:/opt/airflow/test
    ports:
      - "8080:8080"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - orc-ai-network

  # Airflow Scheduler
  airflow-scheduler:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: orc-ai-airflow-scheduler
    command: scheduler
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow
      AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW_FERNET_KEY}
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: 'true'
      OPENAI_API_KEY: ${OPENAI_API_KEY}
      ANTHROPIC_API_KEY: ${ANTHROPIC_API_KEY}
      GOOGLE_API_KEY: ${GOOGLE_API_KEY}
      MISTRAL_API_KEY: ${MISTRAL_API_KEY}
    volumes:
      - ${AIRFLOW_PROJ_DIR:-.}/podman/code:/opt/airflow/dags
      - ${AIRFLOW_PROJ_DIR:-.}/podman/code/plugins:/opt/airflow/plugins
      - ${AIRFLOW_PROJ_DIR:-.}/podman/temp:/opt/airflow/logs
      - ${AIRFLOW_PROJ_DIR:-.}/podman/data:/opt/airflow/data
      - ${AIRFLOW_PROJ_DIR:-.}/podman/model:/opt/airflow/models
      - ${AIRFLOW_PROJ_DIR:-.}/podman/test:/opt/airflow/test
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8974/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - orc-ai-network

  # Airflow Worker (Celery)
  airflow-worker:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: orc-ai-airflow-worker
    command: celery worker
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow
      AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW_FERNET_KEY}
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      DUMB_INIT_SETSID: "0"
      OPENAI_API_KEY: ${OPENAI_API_KEY}
      ANTHROPIC_API_KEY: ${ANTHROPIC_API_KEY}
      GOOGLE_API_KEY: ${GOOGLE_API_KEY}
      MISTRAL_API_KEY: ${MISTRAL_API_KEY}
    volumes:
      - ${AIRFLOW_PROJ_DIR:-.}/podman/code:/opt/airflow/dags
      - ${AIRFLOW_PROJ_DIR:-.}/podman/code/plugins:/opt/airflow/plugins
      - ${AIRFLOW_PROJ_DIR:-.}/podman/temp:/opt/airflow/logs
      - ${AIRFLOW_PROJ_DIR:-.}/podman/data:/opt/airflow/data
      - ${AIRFLOW_PROJ_DIR:-.}/podman/model:/opt/airflow/models
      - ${AIRFLOW_PROJ_DIR:-.}/podman/test:/opt/airflow/test
    healthcheck:
      test: ["CMD-SHELL", "celery --app airflow.executors.celery_executor.app inspect ping -d celery@$${HOSTNAME}"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - orc-ai-network

  # Airflow Flower (Celery monitoring)
  airflow-flower:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: orc-ai-airflow-flower
    command: celery flower
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow
      AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW_FERNET_KEY}
    ports:
      - "5555:5555"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:5555/"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - orc-ai-network

  # Apache Kafka for Event Streaming
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    container_name: orc-ai-zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"
    networks:
      - orc-ai-network

  kafka:
    image: confluentinc/cp-kafka:7.4.0
    container_name: orc-ai-kafka
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'
    volumes:
      - kafka_data:/var/lib/kafka/data
    networks:
      - orc-ai-network

  # Prometheus for Metrics Collection
  prometheus:
    image: prom/prometheus:v2.47.0
    container_name: orc-ai-prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=200h'
      - '--web.enable-lifecycle'
    volumes:
      - ./podman/config/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    ports:
      - "9090:9090"
    networks:
      - orc-ai-network

  # Grafana for Visualization
  grafana:
    image: grafana/grafana:10.1.0
    container_name: orc-ai-grafana
    environment:
      GF_SECURITY_ADMIN_USER: ${GRAFANA_USER:-admin}
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_PASSWORD:-admin}
      GF_USERS_ALLOW_SIGN_UP: 'false'
    volumes:
      - grafana_data:/var/lib/grafana
      - ./podman/config/grafana/provisioning:/etc/grafana/provisioning
    ports:
      - "3000:3000"
    networks:
      - orc-ai-network

  # Dask Scheduler for Parallel Computation
  dask-scheduler:
    image: daskdev/dask:2023.9.2
    container_name: orc-ai-dask-scheduler
    command: ["dask-scheduler"]
    ports:
      - "8786:8786"
      - "8787:8787"
    networks:
      - orc-ai-network

  # Dask Worker
  dask-worker:
    image: daskdev/dask:2023.9.2
    container_name: orc-ai-dask-worker
    command: ["dask-worker", "dask-scheduler:8786"]
    depends_on:
      - dask-scheduler
    environment:
      DASK_WORKER_MEMORY_LIMIT: 2GB
    volumes:
      - ${AIRFLOW_PROJ_DIR:-.}/podman/data:/data
      - ${AIRFLOW_PROJ_DIR:-.}/podman/temp:/tmp
    networks:
      - orc-ai-network

  # LocalStack for AWS Service Emulation
  localstack:
    image: localstack/localstack:2.3.0
    container_name: orc-ai-localstack
    environment:
      SERVICES: s3,sqs,sns,lambda,dynamodb
      DEBUG: 1
      DATA_DIR: /tmp/localstack/data
      AWS_DEFAULT_REGION: us-east-1
      AWS_ACCESS_KEY_ID: test
      AWS_SECRET_ACCESS_KEY: test
    ports:
      - "4566:4566"
    volumes:
      - localstack_data:/tmp/localstack
      - /var/run/docker.sock:/var/run/docker.sock
    networks:
      - orc-ai-network

  # Keycloak for Identity and Access Management
  keycloak:
    image: quay.io/keycloak/keycloak:22.0.1
    container_name: orc-ai-keycloak
    environment:
      KEYCLOAK_ADMIN: ${KEYCLOAK_ADMIN:-admin}
      KEYCLOAK_ADMIN_PASSWORD: ${KEYCLOAK_ADMIN_PASSWORD:-admin}
      KC_DB: postgres
      KC_DB_URL: jdbc:postgresql://postgres:5432/keycloak
      KC_DB_USERNAME: ${POSTGRES_USER:-airflow}
      KC_DB_PASSWORD: ${POSTGRES_PASSWORD:-airflow}
    command: start-dev
    ports:
      - "8090:8080"
    depends_on:
      - postgres
    networks:
      - orc-ai-network

  # Streamlit Dashboard
  streamlit-dashboard:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: orc-ai-streamlit
    command: streamlit run /app/dashboard/main.py --server.port=8501 --server.address=0.0.0.0
    environment:
      AIRFLOW_API_URL: http://airflow-webserver:8080
      NEO4J_URI: bolt://neo4j:7687
      NEO4J_USER: ${NEO4J_USER:-neo4j}
      NEO4J_PASSWORD: ${NEO4J_PASSWORD:-password}
      PROMETHEUS_URL: http://prometheus:9090
    volumes:
      - ${AIRFLOW_PROJ_DIR:-.}/podman/code/dashboard:/app/dashboard
      - ${AIRFLOW_PROJ_DIR:-.}/podman/data:/app/data
    ports:
      - "8501:8501"
    depends_on:
      - airflow-webserver
      - neo4j
      - prometheus
    networks:
      - orc-ai-network

  # Init container for setup tasks
  airflow-init:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: orc-ai-airflow-init
    entrypoint: /bin/bash
    command:
      - -c
      - |
        function ver() {
          printf "%04d%04d%04d%04d" $${1//./ }
        }
        airflow_version=$$(AIRFLOW__LOGGING__LOGGING_LEVEL=INFO airflow version)
        airflow_version_comparable=$$(ver $${airflow_version})
        min_airflow_version=2.2.0
        min_airflow_version_comparable=$$(ver $${min_airflow_version})
        if (( airflow_version_comparable < min_airflow_version_comparable )); then
          echo -e "\033[1;31mERROR!!!: Too old Airflow version $${airflow_version}!\e[0m"
          echo "The minimum Airflow version supported: $${min_airflow_version}. Only use this or higher!"
          exit 1
        fi
        if [[ -z "$${AIRFLOW_UID}" ]]; then
          echo -e "\033[1;33mWARNING!!!: AIRFLOW_UID not set!\e[0m"
          echo "If you are on Linux, you SHOULD follow the instructions below to set "
          echo "AIRFLOW_UID environment variable, otherwise files will be owned by root."
          echo "For other operating systems you can get rid of the warning with manually created .env file:"
          echo "    See: https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#setting-the-right-airflow-user"
        fi
        one_meg=1048576
        mem_available=$$(($$(getconf _PHYS_PAGES) * $$(getconf PAGE_SIZE) / one_meg))
        cpus_available=$$(grep -cE 'cpu[0-9]+' /proc/stat)
        disk_available=$$(df / | tail -1 | awk '{print $$4}')
        warning_resources="false"
        if (( mem_available < 4000 )) ; then
          echo -e "\033[1;33mWARNING!!!: Not enough memory available for Docker.\e[0m"
          echo "At least 4GB of memory required. You have $$(numfmt --to iec $$((mem_available * one_meg)))"
          warning_resources="true"
        fi
        if (( cpus_available < 2 )); then
          echo -e "\033[1;33mWARNING!!!: Not enough CPUS available for Docker.\e[0m"
          echo "At least 2 CPUs recommended. You have $${cpus_available}"
          warning_resources="true"
        fi
        if (( disk_available < one_meg * 10 )); then
          echo -e "\033[1;33mWARNING!!!: Not enough Disk space available for Docker.\e[0m"
          echo "At least 10 GBs recommended. You have $$(numfmt --to iec $$((disk_available * 1024 )))"
          warning_resources="true"
        fi
        if [[ $${warning_resources} == "true" ]]; then
          echo
          echo -e "\033[1;33mWARNING!!!: You have not enough resources to run Airflow (see above)!\e[0m"
          echo "Please follow the instructions to increase amount of resources available:"
          echo "   https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#before-you-begin"
        fi
        mkdir -p /sources/logs /sources/dags /sources/plugins
        chown -R "$${AIRFLOW_UID}:0" /sources/{logs,dags,plugins}
        exec /entrypoint airflow version
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow
      AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW_FERNET_KEY}
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW_UID: 50000
    volumes:
      - ${AIRFLOW_PROJ_DIR:-.}:/sources
    user: "0:0"
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy

networks:
  orc-ai-network:
    driver: bridge

volumes:
  postgres_data:
  neo4j_data:
  neo4j_logs:
  neo4j_conf:
  neo4j_import:
  prometheus_data:
  grafana_data:
  kafka_data:
  localstack_data: